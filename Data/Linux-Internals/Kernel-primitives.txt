Kthread:
========
> It is often useful for the kernel to perform some operations in the background. The
  kernel accomplishes this via kernel threads.
> The significant difference between kernel threads and normal processes is that
  kernel threads do not have an address space. (Their mm pointer, which points at their
  address space, is NULL .)
> Kernel threads are schedulable and preemptable, the same as normal processes.
> Kernel threads are created on system boot by other kernel threads. Indeed, a kernel 
  thread can be created only by another kernel thread.
> struct task_struct *kthread_create(int (*threadfn)(void *data),
                                     void *data,
                                     const char namefmt[],
                                     ...)
> The process is created in an unrunnable state; it will not start running until 
  explicitly woken up via wake_up_process().
> A process can be created and made runnable with a single function, kthread_run():
  struct task_struct *kthread_run(int (*threadfn)(void *data),
                                  void *data,
                                  const char namefmt[],
                                  ...)
> When started, a kernel thread continues to exist until it calls do_exit() or another
  part of the kernel calls kthread_stop(), passing in the address of the 
  task_struct structure returned by kthread_create():
  int kthread_stop(struct task_struct *k)
  
  
SoftIRQs:
=========
> Softirqs are statically allocated at compile time. Unlike tasklets, you cannot 
  dynamically register and destroy softirqs. Softirqs are represented by the 
  softirq_action structure.
  struct softirq_action {
  void (*action)(struct softirq_action *);
  };
> The reason behind making softirq only as static is that we need to add the name
  of softirq(say mysoftirq) to an enum in a file called "interrupt.h", and when the
  compilation of code happens, a new instance of that particular enum(mysoftirq) is
  created, which will be added to the softirq list as is marked as pending, which will
  be processed by the do_irq() routine.
  There is one more reason for not allowing this to be dynamically defined is that
  most of the modules are dynamic drivers and if modules start using softirq, and
  every softirq will use lock and for every driver there will be some interrupt,
  so system will spend lots of time in the interrupt context. That's the reason
  they(kernel guys) restricted it from using it freely. They wanted it to be used
  by the developers who are desperate of CONCURRENT execution of bottom half.
> Each registered softirq consumes one entry in the array. Consequently, there are
  registered softirqs.The number of registered softirqs is statically determined
  at compile time and cannot be changed dynamically.
> The kernel enforces a limit of 32 registered softirqs; in the current kernel, 
  however, only nine exist.
> two or more softirqs of the same type may run concurrently on different processors.
Softirq Types:
--------------
Tasklet           Priority       Softirq Description
---------------------------------------------------------
HI_SOFTIRQ           0           High-priority tasklets
TIMER_SOFTIRQ        1           Timers
NET_TX_SOFTIRQ       2           Send network packets
NET_RX_SOFTIRQ       3           Receive network packets
BLOCK_SOFTIRQ        4           Block devices
TASKLET_SOFTIRQ      5           Normal priority tasklets
SCHED_SOFTIRQ        6           Scheduler
HRTIMER_SOFTIRQ      7           High-resolution timers
RCU_SOFTIRQ          8           RCU locking

> Softirqs are statically allocated at compile time. Unlike tasklets, you cannot 
  dynamically register and destroy softirqs. Softirqs are represented by the
  softirq_action structure, which is defined in <linux/interrupt.h>
    struct softirq_action                                                           
    {                                                                               
        void (*action)(struct softirq_action *);                                 
    };

> A 32-entry array of this structure is declared in kernel/softirq.c
    static struct softirq_action softirq_vec[NR_SOFTIRQS];
> Each registered softirq consumes one entry in the array. Consequently, there are
  registered softirqs.The number of registered softirqs is statically determined
  at compile time and cannot be changed dynamically.The kernel enforces a limit of 32
  registered softirqs
> The prototype of a softirq handler, action , looks like
    void softirq_handler(struct softirq_action *)
> A softirq never preempts another softirq.The only event that can preempt a softirq
  is an interrupt handler.Another softirq—even the same one—can run on another
  processor, however.
> A registered softirq must be marked before it will execute.This is called raising
  the softirq. Usually, an interrupt handler marks its softirq for execution before 
  returning. Then, at a suitable time, the softirq runs. Pending softirqs are checked 
  for and executed in the following places:
    In the return from hardware interrupt code path
    In the ksoftirqd kernel thread
    In any code that explicitly checks for and executes pending softirqs, such as 
    the networking subsystem.
> Regardless of the method of invocation, softirq execution occurs in __do_softirq(),
  which is invoked by do_softirq().
> 






The key differences between softirq and tasklet are:
----------------------------------------------------
Allocation
----------
Softirqs are statically allocated at compile-time. Unlike tasklets, you cannot 
dynamically register and destroy softirqs.
Tasklets can be statically allocated using DECLARE_TASKLET(name, func, data) or can 
also be allocated dynamically and initialized at runtime using 
'tasklet_init(name, func, data)'

Concurrency
-----------
Softirqs can run concurrently on several CPUs, even if they are of the same type 
because softirqs are reentrant functions and must explicitly protect their data 
structures with spinlocks.
Tasklets are non-reentrant and tasklets of the same type are always serialized, 
in other words, the same type of tasklet cannot be executed by two CPUs at the 
same time. However, tasklets of different types can be executed concurrently on 
several CPUs.

Processing
----------
Softirqs are activated by means of the raise_softirq(). The pending softirqs are
processed by do_softirq() and ksoftirqd kernel thread after being enabled by 
'local_bh_enable()' or by 'spin_unlock_bh()'
Tasklets are a bottom-half mechanism built on top of softirqs i.e. tasklets are 
represented by two softirqs: HI_SOFTIRQ and TASKLET_SOFTIRQ. Tasklets are actually run
from a softirq. The only real difference in these types is that the HI_SOFTIRQ based 
tasklets run prior to the TASKLET_SOFTIRQ tasklets. So, tasklet_schedule() basically
calls raise_softirq(TASKLET_SOFTIRQ) Note that softirqs (and hence tasklets and timers)
are run on return from hardware interrupts, or on return from a system call. Also as
soon as the thread that raised the softirq ends, that single softirq (and on other) is
run to minimize softirq latency.


Tasklets:
=========
Tasklets are dynamically created, easy to use, and quick. Moreover.

> Tasklets are bottom half mechanism on top of Softirqs.
> Tasklets are represented by two softirq: HI_SOFTIRQ and TASKLET_SOFTIRQ 
  (HI_SOFTIRQ have highest priority)
> Tasklet structure:
    struct tasklet_struct
    {
        struct tasklet_struct *next;  /* The next tasklet in line for scheduling */
        unsigned long state;          /* TASKLET_STATE_SCHED or TASKLET_STATE_RUN */
        atomic_t count;        /* Responsible for the tasklet being activated or not */
        void (*func)(unsigned long);  /* The main function of the tasklet */
        unsigned long data;           /* The parameter func is started with */
    };

> state: TASKLET_STATE_SCHED, TASKLET_STATE_RUN (TASKLET_STATE_RUN is used only on 
  multiprocessor machine because uniprocessor always knows whether tasklet is running.)
> Count: If it is non zero, the tasklet is disabled and can not run; if it is zero the 
  tasklet is enabled and can run if marked as pending.
> Scheduled tasklets are stored in two pre-processor structure: 
  tasklet_vec(for reguler tasklet) & tasklet_hi_vec(for high-priority tasklet)
> Tasklets are schedule via tasklet_schedule() & tasklet_hi_schedule()
> Trerdown process of tasklet: 
    1. check whether the tasklet's state is TASKLET_STATE_SCHED. If it is, the tasklet
       is already schedule to run, the function can immediately return.
    2. Call __tasklet_schedule() .
    3. Save the state of the interrupt system, and then disable local interrupts. This
       ensures that nothing on this processor will mess with the tasklet code while
       tasklet_schedule() is manipulating the tasklets.
    4. Add the tasklet to be scheduled to the head of the tasklet_vec or 
       tasklet_hi_vec linked list, which is unique to each processor in the system.
    5. Raise the TASKLET_SOFTIRQ or HI_SOFTIRQ softirq, so do_softirq() executes this
       tasklet in the near future.
    6. Restore interrupts to their previous state and return.
> Teardown of handlers, tasklet_action() and tasklet_hi_action() processing:
    1.  Disable local interrupt delivery (there is no need to first save their state
        because the code here is always called as a softirq handler and interrupts are
        always enabled) and retrieve the tasklet_vec or tasklet_hi_vec list for this
        processor.
    2.  Clear the list for this processor by setting it equal to NULL.
    3.  Enable local interrupt delivery.Again, there is no need to restore them to
        their pre-vious state because this function knows that they were always 
        originally enabled.
    4.  Loop over each pending tasklet in the retrieved list.
    5.  If this is a multiprocessing machine, check whether the tasklet is running on
        another processor by checking the TASKLET_STATE_RUN flag. If it is currently
        run-ning, do not execute it now and skip to the next pending tasklet.
        (Recall that only one tasklet of a given type may run concurrently.)
    6.  If the tasklet is not currently running, set the TASKLET_STATE_RUN flag, so 
        another processor will not run it.
    7.  Check for a zero count value, to ensure that the tasklet is not disabled. If
        the tasklet is disabled, skip it and go to the next pending tasklet.
    8.  We now know that the tasklet is not running elsewhere, is marked as running so
        it will not start running elsewhere, and has a zero count value. Run the 
        tasklet handler.
    9.  After the tasklet runs, clear the TASKLET_STATE_RUN flag in the tasklet’s state
        field.
    10. Repeat for the next pending tasklet, until there are no more scheduled tasklets
        waiting to run.
    
> #define DECLARE_TASKLET(my_tasklet, my_tasklet_handler, dev)    \
                          struct tasklet_struct my_tasklet = { NULL, 0, 
                                                               ATOMIC_INIT(0), \
                                                               my_tasklet_handler, \
                                                               dev };
> #define DECLARE_TASKLET_DISABLED(my_tasklet, my_tasklet_handler, dev)    \
                            struct tasklet_struct my_tasklet = { NULL, 0, \
                                                                 ATOMIC_INIT(1),\
                                                                 my_tasklet_handler, \
                                                                 dev };

> You can create tasklets statically or dynamically.What option you choose depends on 
  whether you have (or want) a direct or indirect reference to the tasklet. If you are
  going to statically create the tasklet (and thus have a direct reference to it)
> As with softirqs, tasklets cannot sleep.This means you cannot use semaphores or other
  blocking functions in a tasklet.
> Tasklets also run with all interrupts enabled.
> If the same tasklet is scheduled again, before it has had a chance to run, it still 
  runs only once. If it is already running, for example on another processor, the 
  tasklet is rescheduled and runs again.
> You can disable a tasklet via a call to tasklet_disable(), which disables the given 
  tasklet. If the tasklet is currently running, the function will not return until it 
  finishes exe-cuting.
> Alternatively, you can use tasklet_disable_nosync(), which disables the given tasklet
  but does not wait for the tasklet to complete prior to returning. This is usually not
  safe because you cannot assume the tasklet is not still running.
> You can remove a tasklet from the pending queue via tasklet_kill().
> This function first waits for the tasklet to finish executing and then it removes the
  tasklet from the queue. This function must not be used from interrupt context because
  it sleeps.
> The tasklet_kill_immediate is used only when a given CPU is in the dead state.

APIS:
Tasklet creation and enable/disable functions:
----------------------------------------------
DECLARE_TASKLET( name, func, data );            /* Creating Static tasklet*/
DECLARE_TASKLET_DISABLED( name, func, data);    /*Static tasklet disable*/
void tasklet_init( struct tasklet_struct *, 
                   void (*func)(unsigned long), 
                   unsigned long data );      /*creating tasklet dynamically*/
void tasklet_disable_nosync(struct tasklet_struct *t); /* disabling */
void tasklet_disable(struct tasklet_struct *t);    /* disabling with the wait for 
                                                      the completion of tasklet’s 
                                                      operation */
void tasklet_enable(struct tasklet_struct *t);     /* enabling */
void tasklet_hi_enable( struct tasklet_struct * ); /*Enabling high priority tasklet*/

Tasklet scheduling functions:
-----------------------------
void tasklet_schedule(struct tasklet_struct *t);           /* with normal priority */
void tasklet_hi_schedule(struct tasklet_struct *t);        /* with high priority */
void tasklet_hi_schedule_first(struct tasklet_struct *t);  /* out of the queue */

Tasklet kill functions:
-----------------------
void tasklet_kill( struct tasklet_struct * );
void tasklet_kill_immediate( struct tasklet_struct *, unsigned int cpu );


Questions:
----------
1. Difference between static and dynamic tasklet.

Observation while experiment on tasklet:
----------------------------------------
1. Create dynamic tasklet, disable it and shcedule the same with 'tasklet_hi_schedule'.
   In module exit function i am not killing the tasklet as it is not enable. Insmode is
   working fine but kernel got crash when i attempt to unload the module.
2. Create dynamic tasklet, disable it and shcedule the same with 'tasklet_hi_schedule'.
   In module exit function i am killing the tasklet with 'tasklet_kill'. Insmode is 
   working fine but it hang the terminal when i attempt to unload the module. 
   This might be because 'tasklet_kill' is blocking call, since tasklet is not enable,
   therefore it won't be schedule and hence the 'tasklet_kill' will wait to kill the 
   tasklet which is even not schedule.
   

Work Queues:
============
> We can create a queue with the following flags:
    WQ_HIGHPRI
    WQ_UNBOUND
    WQ_CPU_INTENSIVE
    WQ_FREEZABLE
    WQ_MEM_RECLAIM
> Work Queues are the only bottom-half mechanisms that run in process context, and 
  thus, the only ones that can sleep.
> This means they are useful for situations in which you need to allocate a lot of 
  memory, obtain a semaphore, or perform block I/O.
> The work queue subsystem is an interface for creating kernel threads to handle work 
  queued from elsewhere.These kernel threads are called worker threads.
> Tasklets are handled by the scheduling function, and workqueues are processed by
  special threads called workers.
> The default worker threads are called events/n where n is the processor number;
  there is one per processor. For example, on a uniprocessor system there is one 
  thread, events/0.
> struct workqueue_struct {
    struct cpu_workqueue_struct cpu_wq[NR_CPUS];    /*The cpu_workqueue_struct is the 
                                                      core data structure*/
    struct list_head list;
    const char *name;
    int singlethread;
    int freezeable;
    int rt;
  };
  
> struct cpu_workqueue_struct {
    spinlock_t lock;                /* Spin lock used to protect the structure */
    struct list_head worklist;      /* Head of the list of pending functions */
    wait_queue_head_t more_work;    /* Wait queue where the worker thread waiting for 
                                       more work to be done sleeps */
    struct work_struct *current_struct;
    struct workqueue_struct *wq;        /* Pointer to the workqueue_struct structure 
                                           containing this descriptor */
    task_t *thread;                     /* Process descriptor pointer of the worker 
                                           thread of the structure */
  };

> All worker threads are implemented as normal kernel threads running the 
  worker_thread() function.
> After initial setup, this function enters an infinite loop and goes to sleep. When 
  work is queued, the thread is awakened and processes the work. When there is no work 
  left to process, it goes back to sleep.
> All of these kernel threads are called worker threads. The work queue are maintained
  by the 'work_struct'
> struct work_struct {                                                            
    atomic_long_t data;                                                         
    struct list_head entry;                                                     
    work_func_t func;                                                           
    #ifdef CONFIG_LOCKDEP                                                           
    struct lockdep_map lockdep_map;                                             
    #endif                                                                          
  };

> These structures are strung into a linked list, one for each type of queue on each 
  processor. For example, there is one list of deferred work for the generic thread,
  per processor.When a worker thread wakes up, it runs any work in its list. As it 
  completesWork Queues work, it removes the corresponding work_struct entries from the
  linked list.When the list is empty, it goes back to sleep.
> heart of worker_thread(), simplified:
    for (;;) {
        prepare_to_wait(&cwq->more_work, &wait, TASK_INTERRUPTIBLE); /* The thread 
                                         marks itself sleeping (the task’s state is 
                                         set to TASK_INTERRUPTIBLE) and adds itself 
                                         to a wait queue. */
        if (list_empty(&cwq->worklist))   /* If the linked list of work is empty, 
                                             the thread calls schedule() and goes to 
                                             sleep. */
        schedule();
        finish_wait(&cwq->more_work, &wait);  /* If the list is not empty, the thread
                                                 does not go to sleep. Instead, it 
                                                 marks itself TASK_RUNNING and removes
                                                 itself from the wait queue. */
        run_workqueue(cwq);     /* If the list is nonempty, the thread calls 
                                   run_workqueue() to perform the deferred work. */
    }

> The function run_workqueue() , in turn, actually performs the deferred work:
    while (!list_empty(&cwq->worklist)) {   /* While the list is not empty, it grabs 
                                               the next entry in the list. */
        struct work_struct *work;
        work_func_t f;
        void *data;
        work = list_entry(cwq->worklist.next, struct work_struct, entry);  /* It 
                                              retrieves the function that should be 
                                              called, func , and its argument, 
                                              data. */
        f = work->func;
        list_del_init(cwq->worklist.next);
        work_clear_pending(work);     /* It removes this entry from the list and 
                                         clears the pending bit in the structure 
                                         itself. */
        f(work);                      /* It invokes the function. */
    }


worker thread <--------------------------- cpu_workqueue_struct /* one per processor */
/*(events, falcon)*/             ------------------->^
                                |                    |
                                |                    |
                                |                    |
                                |                    |
                                |                    |
                                |         workqueue_struct structure /* one per type 
                                |                    of worker thread (one for events 
                                |                    type and one for falcon type) */
                                |          
                                |          
                                |          
                                |          
                                |          
                                --------- work_struct /* one per deferrable 
                                                         function */
                                
> There can be multiple types of worker threads; there is one worker thread per 
  processor of a given type.
> By default, there is the events worker thread. Each worker thread is represented by 
  the cpu_workqueue_struct structure.
> The workqueue_struct structure represents all the worker threads of a given type.
> Your driver creates work, which it wants to defer to later.The work_struct structure 
  represents this work.
> Among other things, this structure contains a pointer to the function that handles 
  the deferred work.
> The work is submitted to a specific worker thread—in this case, a specific falcon 
  thread.The worker thread then wakes up and performs the queued work.

APIs:
Creating work:
--------------
DECLARE_WORK(name, void (*func)(void *));  /* This statically creates a work_struct 
                                              structure named name with handler 
                                              function 'func'. */

INIT_WORK(struct work_struct *work, void (*func)(void *));  /* This dynamically 
                             creates a work at runtime via a pointer. This dynamically 
                             initializes the work queue pointed to by work with 
                             handler function 'func'. */

Work Queue Handler:
-------------------
void work_handler(struct work_struct *work)

> A worker thread executes this function, and thus, the function runs in process 
  context.
> By default, interrupts are enabled and no locks are held. If needed, the function can
  sleep. Note that, despite running in process context, 
  the work handlers cannot access user-space memory because there is no associated 
  user-space memory map for kernel threads.
> The kernel can access user memory only when running on behalf of a user-space 
  process, such as when executing a system call. Only then is user memory mapped in.

Scheduling Work:
----------------
schedule_work(&work);   /* The work is scheduled immediately and is run as soon as the
                           events worker thread on the current processor wakes up. */
static inline bool schedule_delayed_work_on(int cpu, 
                                            struct delayed_work *dwork,
                                            unsigned long delay)  /* After waiting for
                                            a given time this puts a job in the 
                                            kernel-global workqueue on the specified 
                                            CPU. */
struct delayed_work {                                                           
    struct work_struct work;                                                    
    struct timer_list timer;                                                                              
    /* target workqueue and CPU ->timer uses to queue ->work */                 
    struct workqueue_struct *wq;                                                
    int cpu;                                                                    
};

Flushing Work:
--------------
> Sometimes, we need to ensure that a given batch of work has completed before 
  continuing.This is especially important for modules, which almost 
  certainly want to call this function before unloading. Other places in the kernel 
  also might need to make certain no work is pending, to prevent 
  race conditions.

void flush_scheduled_work(void);    /* This function waits until all entries in the 
                                       queue are executed before returning. While 
                                       waiting for any pending work to execute, the 
                                       function sleeps.Therefore, you can call it only 
                                       from process context. */

> This function does not cancel any delayed work.That is, any work that was scheduled 
  via schedule_delayed_work(), and whose delay is not yet up, is  not flushed via 
  'flush_scheduled_work()'.

int cancel_delayed_work(struct work_struct *work); /* This function cancels the 
                                                      pending work, if any, associated
                                                      with the given work_struct. */

Creating New Work Queues:
-------------------------
> If the default queue is insufficient for your needs, you can create a new work queue 
  and corresponding worker threads. Because this creates one worker thread per 
  processor, you should create unique work queues only if your code needs the 
  performance of a unique set of threads.
  
struct workqueue_struct *create_workqueue(const char *name);  /* This function creates 
                         all the worker threads (one for each processor in the system)
                         and prepares them to handle work. */

> The parameter name is used to name the kernel threads. For example, the default 
  events queue is created via 
  struct workqueue_struct *keventd_wq;
  keventd_wq = create_workqueue(“events”);

> After the work is created, the following functions are analogous to schedule_work() 
  and schedule_delayed_work(), except that they work on the given work queue and not 
  the default events queue.
  
int queue_work(struct workqueue_struct *wq, struct work_struct *work)

int queue_delayed_work(struct workqueue_struct *wq, struct work_struct *work, 
                       unsigned long delay)

> Finally, you can flush a wait queue via a call to the function

flush_workqueue(struct workqueue_struct *wq)  /* As previously discussed, this 
                     function works identically to flush_scheduled_work(), except that
                     it waits for the given queue to empty before returning. */

Which Bottom Half Should I Use?
-------------------------------
> A driver developer should always choose tasklets over softirqs, unless prepared to 
  utilize per-processor variables or similar magic to ensure that the softirq can 
  safely run concurrently on multiple processors.
> If your deferred work needs to run in process context, your only choice of the three 
  is work queues.
> 
  
Kernel-Synchronization:
=======================

> Concurrent access of shared data is a recipe for instability.
> Code that is safe from concurrent access from an interrupt handler is said to be interrupt-safe.
> Code that is safe from concurrency on symmetrical multiprocessing machines is SMP-safe.
> Code that is safe from concurrency with kernel preemption is preempt-safe.
> Whenever you write kernel code, you should ask yourself these questions:
 1. Is the data global? Can a thread of execution other than the current one access it?
 2. Is the data shared between process context and interrupt context? Is it shared between  
           two different interrupt handlers?
 3. If a process is preempted while accessing this data, can the newly scheduled process 
           access the same data?
 4. Can the current process sleep (block) on anything? If it does, in what state does that 
           leave any shared data?
 5. What prevents the data from being freed out from under me?
 6. What happens if this function is called again on another processor?
 7. Given the proceeding points, how am I going to ensure that my code is safe from  
           concurrency?

> Prevention of deadlock scenarios is important.Although it is difficult to prove that code
is free of deadlocks, you can write deadlock-free code.A few simple rules go a long way:
 1. Implement lock ordering. Nested locks must always be obtained in the same order.
 2. This prevents the deadly embrace deadlock. Document the lock ordering so others will follow it.
 3. Prevent starvation.Ask yourself, does this code always finish? If foo does not occur, will bar wait forever?
 4. Do not double acquire the same lock.
 6. Design for simplicity. Complexity in your locking scheme invites deadlocks.

Synchronization Method:
=======================

1. Atomic Operation
-------------------
2. Spin lock:
-------------
> A spin lock is a lock that can be held by at most one thread of execution. If a 
  thread of execution attempts to acquire a spin lock while it is already held, which
  is called contended, the thread busy loops spins—waiting for the lock to become 
  available. If the lock is not contended, the thread can immediately acquire the 
  lock and continue.
> APIs:
 DEFINE_SPINLOCK(mr_lock);
 spin_lock(&mr_lock);
 /* critical region ... */
 spin_unlock(&mr_lock);
 
> The lock can be held simultaneously by at most only one thread of execution. 
  Consequently, only one thread is allowed in the critical region at a time.
> On uniprocessor machines, the locks compile away and do not exist; they simply act 
  as markers to disable and enable kernel preemption. If kernel preempt is turned 
  off, the locks compile away entirely.
> Spin Locks Are Not Recursive: Unlike spin lock implementations in other operating 
  systems and threading libraries, the Linux kernel’s spin locks are not recursive. 
  This means that if you attempt to acquire a lock you already hold, you will spin, 
  waiting for yourself to release the lock. But because you are busy spinning, you 
  will never release the lock and you will deadlock. Be careful!
> Spin locks can be used in interrupt handlers, whereas semaphores cannot be used 
  because they sleep. If a lock is used in an interrupt handler, you must also 
  disable local interrupts (interrupt requests on the current processor) before 
  obtaining the lock. Otherwise, it is possible for an interrupt handler to interrupt
  kernel code while the lock is held and attempt to reacquire the lock. The interrupt 
  handler spins, waiting for the lock to become available.The lock holder, however, 
  does not run until the interrupt handler completes. This is an example of the 
  double-acquire deadlock.
> The kernel provides an interface that conveniently disables interrupts and acquires
  the lock. Usage is
 DEFINE_SPINLOCK(mr_lock);
 unsigned long flags;
 spin_lock_irqsave(&mr_lock, flags);
 /* critical region ... */
 spin_unlock_irqrestore(&mr_lock, flags);

> The lock and unlock also disable and enable kernel preemption, respectively.
> If you always know before the fact that interrupts are initially enabled, there is 
  no need to restore their previous state.You can unconditionally enable them on 
  unlock. In those cases, spin_lock_irq() and spin_unlock_irq() are optimal:
 DEFINE_SPINLOCK(mr_lock);
 spin_lock_irq(&mr_lock);
 /* critical section ... */
 spin_unlock_irq(&mr_lock);

> As the kernel grows in size and complexity, it is increasingly hard to ensure that
  interrupts are always enabled in any given code path in the kernel. Use of 
  spin_lock_irq() therefore is not recommended.
> Debugging Spin Locks: The configure option CONFIG_DEBUG_SPINLOCK enables a handful 
  of debugging checks in the spin lock code. For example, with this option the spin 
  lock code checks for the use of uninitialized spin locks and unlocking a lock that 
  is not yet locked. When testing your code, you should always run with spin lock 
  debugging enabled. For additional debugging of lock lifecycles, 
  enable CONFIG_DEBUG_LOCK_ALLOC .
  
Other Spin Lock Methods:
------------------------
Spin Lock Methods                 Method Description
-------------------------------------------------------------------------------------
spin_lock()                       Acquires given lock
spin_lock_irq()                   Disables local interrupts and acquires given lock
spin_lock_irqsave()               Saves current state of local interrupts, disables 
                                  local interrupts, and acquires given lock
spin_unlock()                     Releases given lock
spin_unlock_irq()                 Releases given lock and enables local interrupts
spin_unlock_irqrestore()          Releases given lock and restores local interrupts 
                                  to given previous state
spin_lock_init()                  Dynamically initializes given spinlock_t
spin_trylock()                    Tries to acquire given lock; if unavailable, 
                                  returns nonzero
spin_is_locked()                  Returns nonzero if the given lock is currently 
                                  acquired, otherwise it returns zero
> The function spin_lock_bh() obtains the given lock and disables all bottom halves.
  The function spin_unlock_bh() performs the inverse.
> If the data is shared between two different tasklets, however, you must obtain a 
  normal spin lock before accessing the data in the bottom half.You do not need to 
  disable bottom halves because a tasklet never preempts another running tasklet on 
  the same processor.
> f data is shared by softirqs, it must be protected with a lock. Recall that 
  softirqs, even two of the same type, might run simultaneously on multiple 
  processors in the system. A softirq never preempts another softirq running on the 
  same processor, however, so disabling bottom halves is not needed.

Timers and Time Management:
===========================
> Events that occur periodically—say, every 10 milliseconds—are driven by the 
  system timer. The system timer is a programmable piece of hardware that issues an 
  interrupt at a fixed frequency.The interrupt handler for this timer—called the timer
  interrupt—updates the system time and performs periodic work.
> dynamic timers, the facility used to schedule events that run once after a 
  specified time has elapsed.
> The system timer goes off (often called hitting or popping) at a preprogrammed
  frequency, called the tick rate. When the system timer goes off, it issues an 
  interrupt that the kernel handles via a special interrupt handler.
> Because the kernel knows the preprogrammed tick rate, it knows the time between
  any two successive timer interrupts.This period is called a tick and is equal to 
  1/(tick rate) seconds.
> The frequency of the system timer (the tick rate) is programmed on system boot based
  on a static preprocessor define, HZ .The value of HZ differs for each supported 
  architecture. On some supported architectures, it even differs between machine 
  types.
> by default the x86 architecture defines HZ to be 100.Therefore, the timer interrupt
  on i386 has a frequency of 100HZ and occurs 100 times per second.
> Tickless OS: When a kernel is built with the CONFIG_HZ configuration option set, 
  the system dynamically schedules the timer interrupt in accordance with pending 
  timers. Instead of firing the timer interrupt every, say, 1ms, the interrupt is 
  dynamically scheduled and rescheduled as needed.
> Jiffies: The global variable jiffies holds the number of ticks that have occurred 
  since the system booted. On boot, the kernel initializes the variable to zero, and 
  it is incremented by one during each timer interrupt.
> The former, converting from seconds to ticks
    unsigned long time_stamp = jiffies;     /* Now */
    unsigned long next_tick = jiffies + 1;  /* one tick from now */
    unsigned long later = jiffies + 5*HZ;   /* five seconds from now */
    unsigned long fraction = jiffies + HZ / 10; /* a tenth of a second from now */

> Note that the jiffies variable is prototyped as unsigned long and that storing 
  it in anything else is incorrect.
> With a tick rate of 100, a 32-bit jiffies variable would overflow in about 497 days.
  With HZ increased to 1000, however, that overflow now occurs in just 49.7 days! 
  If jiffies were stored in a 64-bit variable on all architectures, then for any
  reasonable HZ value the jiffies variable would never overflow in anyone’s lifetime.

Kernel Timers:
--------------
    > Timers are represented by struct timer_list , which is defined in 
      <linux/timer.h> :
    struct timer_list {                                                             
        /*                                                                          
         * All fields that change during normal runtime grouped to the              
         * same cacheline                                                           
         */                                                                         
        struct hlist_node   entry;                                                  
        unsigned long       expires;                                                
        void            (*function)(struct timer_list *);                           
        u32         flags;                                                          
                                                                                    
    #ifdef CONFIG_LOCKDEP                                                           
        struct lockdep_map  lockdep_map;                                            
    #endif                                                                          
    };

> activate the timer:
    add_timer(&my_timer);
> Sometimes you might need to modify the expiration of an already active timer.The 
  kernel implements a function, mod_timer() , which changes the expiration of a given
  timer:
  mod_timer(&my_timer, jiffies + new_delay);    /* new expiration */
> The mod_timer() function can operate on timers that are initialized but not active,
  too. If the timer is inactive, mod_timer() activates it.The function returns zero 
  if the timer were inactive and one if the timer were active. In either case, 
  upon return from mod_timer() , the timer is activated and set to the new expiration.
> If you need to deactivate a timer prior to its expiration, use 
  the del_timer() function:
  del_timer(&my_timer);
> The function works on both active and inactive timers. If the timer is already 
  inactive, the function returns zero; otherwise, the function returns one. Note that
  you do not need to call this for timers that have expired because they are 
  automatically deactivated.
> On a multiprocessing machine, however, the timer handler might already be executing
  on another processor.To deactivate the timer and wait until a potentially executing
  handler for the timer exits, use del_timer_sync() :
    del_timer_sync(&my_timer);
> Unlike del_timer() , del_timer_sync() cannot be used from interrupt context.

Timer Race Conditions:
----------------------
> Because timers run asynchronously with respect to the currently executing code,
  several potential race conditions exist. First, never do the following as a 
  substitute for a mere mod_timer() , because this is unsafe on multiprocessing
  machines:
    del_timer(my_timer)
    my_timer->expires = jiffies + new_delay;
    add_timer(my_timer);

Timer Implementation:
---------------------
> The kernel executes timers in bottom-half context, as softirqs, after the timer 
  interrupt completes.The timer interrupt handler runs update_process_times(), which
  calls run_local_timers() :
    void run_local_timers(void)
    {
        hrtimer_run_queues();
        raise_softirq(TIMER_SOFTIRQ);       /* raise the timer softirq */
        softlockup_tick();
    }
> The TIMER_SOFTIRQ softirq is handled by run_timer_softirq() .This function runs all
  the expired timers (if any) on the current processor.
> Timers are stored in a linked list. However, it would be unwieldy for the kernel to
  either constantly traverse the entire list looking for expired timers, or keep the
  list sorted by expiration value.
> Actually, no approach guarantees that the delay will be for exactly the time 
  requested. Some come extremely close.

Busy Looping:
-------------
> The idea is simple: Spin in a loop until the desired number of clock ticks pass.
  For example
    unsigned long timeout = jiffies + 10;
    /* ten ticks */
    while (time_before(jiffies, timeout))
    ;
> This approach is not nice to the rest of the system. While your code waits, the 
  processor is tied up spinning in a silly loop—no useful work is accomplished!
> A better solution would be to reschedule your process to allow the processor to 
  accomplish other work while your code waits:
    unsigned long delay = jiffies + 5*HZ;
    while (time_before(jiffies, delay))
    cond_resched();
> The call to cond_resched() schedules a new process, but only if need_resched is set.
  In other words, this solution conditionally invokes the scheduler only if there is 
  some more important task to run. Note that because this approach invokes the 
  scheduler, you cannot make use of it from an interrupt handler—only from process 
  context.All these approaches are best used from process context, because interrupt 
  handlers should execute as quickly as possible.

Small Delays:
-------------
  void udelay(unsigned long usecs)
  void ndelay(unsigned long nsecs)
  void mdelay(unsigned long msecs)

> schedule_timeout()
  A more optimal method of delaying execution is to use schedule_timeout(). This call
  puts your task to sleep until at least the specified time has elapsed.There is no 
  guarantee that the sleep duration will be exactly the specified time—only that the 
  duration is at least as long as specified.When the specified time has elapsed, 
  the kernel wakes the task up and places it back on the runqueue. Usage is easy:
    /* set task’s state to interruptible sleep */
    set_current_state(TASK_INTERRUPTIBLE);
    /* take a nap and wake up in “s” seconds */
    schedule_timeout(s * HZ);
> Sometimes it is desirable to wait for a specific event or wait for a specified time 
  to elapse—whichever comes first. In those cases, code might simply call
  schedule_timeout() instead of schedule() after placing itself on a wait queue.
  The task wakes up when the desired event occurs or the specified time elapses.

Interupts and Interrupt handlers:
=================================
> An interrupt is an event raised by software or hardware when it needs the CPU's 
  attention.
> Interrupts enable hardware to signal to the processor. For example, as you type, 
  the keyboard controller (the hardware device that manages the keyboard) issues an 
  electrical signal to the processor to alert the operating system to newly available
  key presses. These electrical signals are interrupts.
> Interrupts associated with devices on the PCI bus, for example, generally are
  dynamically assigned. Other non-PC architectures have similar dynamic assignments
  for interrupt values.
> The important notion is that a specific interrupt is associated with a specific
  device, and the kernel knows this.The hardware then issues interrupts to get the
  kernel’s attention.
> Exceptions: Unlike interrupts, exceptions occur synchronously with respect to the
  processor clock. Indeed, they are often called synchronous interrupts.
> Exceptions are produced by the processor while executing instructions either in 
  response to a programming error (for example, divide by zero) or abnormal conditions
  that must be handled by the kernel (for example, a page fault).
> Interrupts: asynchronous interrupts generated by hardware.
  Exceptions: synchronous interrupts generated by the processor.
> These two goals—that an interrupt handler execute quickly and perform a large amount
  of work—clearly conflict with one another. Because of these competing goals, the 
  processing of interrupts is split into two parts, or halves.The interrupt handler 
  is the top half. The top half is run immediately upon receipt of the interrupt and
  performs only the work that is time-critical, such as acknowledging receipt of the
  interrupt or resetting the hardware. Work that can be performed later is deferred
  until the bottom half.The bottom half runs in the future, at a more convenient time,
  with all interrupts enabled.
> Drivers can register an interrupt handler and enable a given interrupt line for 
  handling with the function request_irq() , which is declared in 
  <linux/interrupt.h> :
    /* request_irq: allocate a given interrupt line */
    int request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags, 
                    const char *name, void *dev);
> Interrupt handler:
  typedef irqreturn_t (*irq_handler_t)(int, void *);
  Note the specific prototype of the handler function: It takes two parameters and 
  has a return value of irqreturn_t.
> Interrupt Handler Flags, can be either zero or a bit mask of one or more of the
  flags defined in <linux/interrupt.h>. Among these flags, the most important are:
  IRQF_DISABLED     : When set, this flag instructs the kernel to disable all 
                      interrupts when executing this interrupt handler.When unset,
                      interrupt handlers run with all interrupts except their own 
                      enabled.
  IRQF_SAMPLE_RANDOM: This flag specifies that interrupts generated by this device
                      should contribute to the kernel entropy pool. Do not set this 
                      if your device issues interrupts at a predictable rate 
                      (for example, the system timer) or can be influenced by external
                      attackers (for example, a networking device).
  IRQF_TIMER        : This flag specifies that this handler processes interrupts for 
                      the system timer.
  IRQF_SHARED       : This flag specifies that the interrupt line can be shared among
                      multiple interrupt handlers. Each handler registered on a given
                      line must specify this flag; otherwise, only one handler can 
                      exist per line.
> void *dev, is used for shared interrupt lines.When an interrupt handler is 
  freed (discussed later), dev provides a unique cookie to enable the removal of only
  the desired interrupt handler from the interrupt line.Without this parameter, it
  would be impossible for the kernel to know which handler to remove on a given 
  interrupt line. You can pass NULL here if the line is not shared, but you must pass
  a unique cookie if your interrupt line is shared. This pointer is also passed into
  the interrupt handler on each invocation.A common practice is to pass the driver’s
  device structure:This pointer is unique and might be useful to have within the
  handlers.
> On success, request_irq() returns zero.A nonzero value indicates an error, in which
  case the specified interrupt handler was not registered.A common error is -EBUSY,
  which denotes that the given interrupt line is already in use (and either the 
  current user or you did not specify IRQF_SHARED).
> Note that request_irq() can sleep and therefore cannot be called from interrupt
  context or other situations where code cannot block. It is a common mistake to call
  request_irq() when it is unsafe to sleep.This is partly because of why request_irq()
  can block: It is indeed unclear.
> On registration, an entry corresponding to the interrupt is created in /proc/irq.
  The function proc_mkdir() creates new procfs entries. This function calls 
  proc_create() to set up the new procfs entries, which in turn calls kmalloc()
  to allocate memory.As you will see in Chapter 12,“Memory Management,” kmalloc()
  can sleep.
> Freeing an Interrupt Handler: When your driver unloads, you need to unregister your
  interrupt handler and potentially disable the interrupt line.To do this, call:
  void free_irq(unsigned int irq, void *dev);
> If the specified interrupt line is not shared, this function removes the handler 
  and disables the line. If the interrupt line is shared, the handler identified via 
  dev is removed, but the interrupt line is disabled only when the last handler is 
  removed. Now you can see why a unique dev is important.With shared interrupt lines,
  a unique cookie is required to differentiate between the multiple handlers that can
  exist on a single line and enable free_irq() to remove only the correct handler.
  In either case (shared or unshared), if dev is non- NULL , it must match the desired
  handler.A call to free_irq() must be made from process context.
> Declaration of an interrupt handler:
  static irqreturn_t intr_handler(int irq, void *dev);
  
> dev , is a generic pointer to the same dev that was given to request_irq() when the
  interrupt handler was registered. If this value is unique (which is required to 
  support sharing), it can act as a cookie to differentiate between multiple
  devices potentially using the same interrupt handler.
> The return value of an interrupt handler is the special type irqreturn_t. An 
  interrupt handler can return two special values, IRQ_NONE or IRQ_HANDLED. The former
  is returned when the interrupt handler detects an interrupt for which its device was
  not the originator.The latter is returned if the interrupt handler was correctly
  invoked, and its device did indeed cause the interrupt.Alternatively, 
  IRQ_RETVAL(val) may be used. If val is nonzero, this macro returns IRQ_HANDLED.
  Otherwise, the macro returns IRQ_NONE. These special values are used to let the 
  kernel know whether devices are issuing spurious (that is, unrequested) interrupts. 
  If all the interrupt handlers on a given interrupt line return IRQ_NONE, then the 
  kernel can detect the problem. Note the curious return type, irqreturn_t, which is 
  simply an int .This value provides backward compatibility with earlier kernels, 
  which did not have this feature.
> The interrupt handler is normally marked static because it is never called directly
  from another file.
> Interrupt handlers in Linux need not be reentrant. When a given interrupt handler 
  is executing, the corresponding interrupt line is masked out on all processors, 
  preventing another interrupt on the same line from being received. Normally all 
  other interrupts are enabled, so other interrupts are serviced, but the current line
  is always disabled. Consequently, the same interrupt handler is never invoked 
  concurrently to service a nested interrupt. This greatly simplifies writing your 
  interrupt handler.
> Interrupt Context: When executing an interrupt handler, the kernel is in 
  interrupt context. Recall that process context is the mode of operation the kernel
  is in while it is executing on behalf of a process—for example, executing a 
  system call or running a kernel thread. In process context, the current macro 
  points to the associated task. Furthermore, because a process is coupled to the 
  kernel in process context, process context can sleep or otherwise invoke the 
  scheduler.
> Interrupt context, on the other hand, is not associated with a process. The current
  macro is not relevant (although it points to the interrupted process). Without a 
  backing process, interrupt context cannot sleep—how would it ever reschedule? 
  Therefore, you cannot call certain functions from interrupt context. If a function 
  sleeps, you cannot use it from your interrupt handler, this limits the functions 
  that one can call from an interrupt handler.
> Interrupt context is time-critical because the interrupt handler interrupts other 
  code. Code should be quick and simple. Busy looping is possible, but discouraged.
  This is an important point; always keep in mind that your interrupt handler has 
  interrupted other code (possibly even another interrupt handler on a 
  different line!). Because of this asynchronous nature, it is imperative that all 
  interrupt handlers be as quick and as simple as possible. As much as possible, 
  work should be pushed out from the interrupt handler and performed in a bottom half,
  which runs at a more convenient time.
> Early in the 2.6 kernel process, an option was added to reduce the stack size from 
  two pages down to one, providing only a 4KB stack on 32-bit systems. This reduced
  memory pressure because every process on the system previously needed two pages of
  contiguous, nonswappable kernel memory.To cope with the reduced stack size, 
  interrupt handlers were given their own stack, one stack per processor, one page 
  in size.This stack is referred to as the interrupt stack.Although the total size 
  of the interrupt stack is half that of the original shared stack, the average stack 
  space available is greater because interrupt handlers get the full page of memory 
  to themselves.
> /proc/interrupts: Procfs is a virtual filesystem that exists only in kernel memory 
  and is typically mounted at /proc . Reading or writing files in procfs invokes 
  kernel functions that simulate reading or writing from a real file.A relevant 
  example is the /proc/interrupts file, which is populated with statistics related 
  to interrupts on the system.
> Disabling and Enabling Interrupts: To disable interrupts locally for the current 
  processor (and only the current processor) and then later reenable them, do the 
  following:
    local_irq_disable();
    /* interrupts are disabled .. */
    local_irq_enable();
> save the state of the interrupt system before disabling it.Then, when you are ready
  to reenable interrupts, you simply restore them to their original state:
    unsigned long flags;
    local_irq_save(flags);
    /* interrupts are now disabled */
    /* ... */
    local_irq_restore(flags); /* interrupts are restored to their previous state */

> Disabling a Specific Interrupt Line: In some cases, it is useful to disable only 
  a specific interrupt line for the entire system.This is called masking out an
  interrupt line.As an example, you might want to disable delivery of a device’s 
  interrupts before manipulating its state. Linux provides four interfaces for 
  this task:
  /* disable a given interrupt line in the interrupt controller. function does not 
     return until any currently executing handler completes. Thus, callers are assured 
     not only that new interrupts will not be delivered on the given line, but also that 
     any already executing handlers have exited. */
  void disable_irq(unsigned int irq);
  
  /* disable a given interrupt line in the interrupt controller. does not wait for 
     current handlers to complete. */ 
  void disable_irq_nosync(unsigned int irq);
  
  /* waits for a specific interrupt handler to exit, if it is executing, before
     returning. */
  void enable_irq(unsigned int irq);
  
  /* Calls to these functions nest. For each call to disable_irq() or 
     disable_irq_nosync() on a given interrupt line, a corresponding call to 
     enable_irq() is required. Only on the last call to enable_irq() is the interrupt
     line actually enabled. */
  void synchronize_irq(unsigned int irq);
  
> Status of the Interrupt System:
  irqs_disabled() : This macro defined in <asm/system.h>, returns nonzero if the
                    interrupt system on the local processor is disabled. Otherwise, 
                    it returns zero.
> Two macros, defined in <linux/hardirq.h>, provide an interface to check the 
  kernel’s current context. They are
  /* It returns nonzero if the kernel is performing any type of interrupt handling.
     This includes either executing an interrupt handler or a bottom half handler. 
     That means If in_interrupt() returns zero, the kernel is in process context
     else interrupt context. */
  in_interrupt()
  /* This macro returns nonzero only if the kernel is specifically executing an 
     interrupt handler.*/
  in_irq()

> Interrupt Control Methods:
    Function               Description
1.  local_irq_disable()    Disables local interrupt delivery
2.  local_irq_enable()     Enables local interrupt delivery
3.  local_irq_save()       Saves the current state of local interrupt delivery and 
                           then disables it
4.  local_irq_restore()    Restores local interrupt delivery to the given state
5.  disable_irq()          Disables the given interrupt line and ensures no handler 
                           on the line is executing before returning
6.  disable_irq_nosync()   Disables the given interrupt line
7.  enable_irq()           Enables the given interrupt line
8.  irqs_disabled()        Returns nonzero if local interrupt delivery is disabled; 
                           otherwise returns zero
9.  in_interrupt()         Returns nonzero if in interrupt context and zero if in 
                           process context
10. in_irq()               Returns nonzero if currently executing an interrupt handler
                           and zero otherwise
                           
> Asynchronous Interrupt:
  • From external source, such as I/O device 
  • Not related to instrucWon being executed 
> Synchronous (also called excep/ons)
  1. Processor-detected excepWons:
     • Faults — correctable; offending instrucWon is retried 
     • Traps — oaen for debugging; instrucWon is not retried 
     • Aborts — major error (hardware failure) 
  2. Programmed excepWons: 
     • Requests for kernel intervenWon (soaware intr/syscalls)
